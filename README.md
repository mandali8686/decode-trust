**DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models**

**Toxicity.** To evaluate how well GPT models avoid generating toxic content.
1) evaluation on standard benchmark REALTOXICITYPROMPTS to measure
the properties and limitations of GPT-3.5 and GPT-4 compared to existing LLM counterparts;

2) evaluation using our manually designed 33 diverse system prompts (e.g., role-playing, saying the
opposite, and replacing word meaning, etc.), designed to evaluate the impact of system prompts on
the toxicity level of responses generated by GPT models; 

3) evaluation on our 1.2K challenging user prompts generated by GPT-4 and GPT-3.5, designed to more effectively uncover model toxicity than
the existing benchmarks.

<img width="800" alt="Screen Shot 2023-11-05 at 6 38 09 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/631ee10f-1f26-476b-9edb-c1fbe113e525">

<img width="800" alt="Screen Shot 2023-11-05 at 6 39 15 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/734f9d02-a368-45bc-b179-87f766fffb22">

**Toxicity Test Design**


<img width="708" alt="Screen Shot 2023-11-05 at 7 08 28 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/bad90fc4-fc8a-4837-b93f-fcf3b44173c2">



**Toxicity Result**


<img width="680" alt="Screen Shot 2023-11-05 at 7 05 09 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/b948e089-c2e8-4509-9acf-ab0c89d0eae4">
<img width="675" alt="Screen Shot 2023-11-05 at 7 05 39 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/ff0c1b50-613c-4cb6-b7b5-27c193205474">
