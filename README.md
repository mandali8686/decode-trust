**DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models**

**Toxicity.** To evaluate how well GPT models avoid generating toxic content.
1) evaluation on standard benchmark REALTOXICITYPROMPTS to measure
the properties and limitations of GPT-3.5 and GPT-4 compared to existing LLM counterparts;

2) evaluation using our manually designed 33 diverse system prompts (e.g., role-playing, saying the
opposite, and replacing word meaning, etc.), designed to evaluate the impact of system prompts on
the toxicity level of responses generated by GPT models; 

3) evaluation on our 1.2K challenging user prompts generated by GPT-4 and GPT-3.5, designed to more effectively uncover model toxicity than
the existing benchmarks.

<img width="500" alt="Screen Shot 2023-11-05 at 6 38 09 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/631ee10f-1f26-476b-9edb-c1fbe113e525">

