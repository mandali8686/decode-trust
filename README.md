**DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models**

__Introduction__
This paper extensive evaluation of the trustworthiness of the latest large language models, particularly GPT-3.5 and GPT-42. The study aims to assess their capabilities and limitations by using a range of benchmarks to measure performance and test resilience against adversarial conditions. Key areas of evaluation include the models' ability to avoid generating toxic content, perpetuation of stereotypes, robustness against adversarial and out-of-distribution data, and their capacity to maintain privacy, ethics, and fairness. The methodology incorporates standard benchmarks, custom datasets, and adversarially crafted prompts. Through rigorous testing, the paper seeks to produce reproducible results that reveal the strengths and weaknesses of these models, contributing to the advancement of reliable, unbiased, and transparent language models that uphold high standards of trustworthiness.

**Toxicity.** To evaluate how well GPT models avoid generating toxic content.
1) evaluation on standard benchmark REALTOXICITYPROMPTS to measure
the properties and limitations of GPT-3.5 and GPT-4 compared to existing LLM counterparts;

2) evaluation using our manually designed 33 diverse system prompts (e.g., role-playing, saying the
opposite, and replacing word meaning, etc.), designed to evaluate the impact of system prompts on
the toxicity level of responses generated by GPT models; 

3) evaluation on our 1.2K challenging user prompts generated by GPT-4 and GPT-3.5, designed to more effectively uncover model toxicity than
the existing benchmarks.

<img width="800" alt="Screen Shot 2023-11-05 at 6 38 09 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/631ee10f-1f26-476b-9edb-c1fbe113e525">

<img width="800" alt="Screen Shot 2023-11-05 at 6 39 15 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/734f9d02-a368-45bc-b179-87f766fffb22">

**Toxicity Test Design**


<img width="708" alt="Screen Shot 2023-11-05 at 7 08 28 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/bad90fc4-fc8a-4837-b93f-fcf3b44173c2">



**Toxicity Result**


<img width="680" alt="Screen Shot 2023-11-05 at 7 05 09 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/b948e089-c2e8-4509-9acf-ab0c89d0eae4">
<img width="675" alt="Screen Shot 2023-11-05 at 7 05 39 PM" src="https://github.com/mandali8686/decode-trust/assets/100242191/ff0c1b50-613c-4cb6-b7b5-27c193205474">
